{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for cleaning data and generationg output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut and paste sakura data \n",
    "# from https://www.data.jma.go.jp/sakura/data/index.html into appropriately named text files\n",
    "# the following steps process the files\n",
    "# here we're using the following file\n",
    "# which contains data gathered on さくら開花, which means sakura flowering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data notes:\n",
    "\n",
    "起日の値は月×100＋日で表現される（例：「1231」は12月31日を指す）\n",
    "date: month*100+day\n",
    "\n",
    "データの属性(rm)=(6,7,8,9)=（代替種目構内観測、代替種目付近観測、正規種目構内観測、正規種目付近観測）\n",
    "\n",
    "Data attribute (rm) = (6, 7, 8, 9) = \n",
    "\n",
    "(Alternative item campus observation, Alternative item near observation,\n",
    "Regular item yard observation, Regular item near observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def process_line(line):\n",
    "    # remove odd characters after location\n",
    "    s_raw = line.replace('\\u3000','')\n",
    "    # match minus following by digit, only replaces minus, sep with comma\n",
    "    s = re.sub('-(?=\\d)', '- ', s_raw) \\\n",
    "        .replace(\" \",\",\") \\\n",
    "        .split(\",\"); \n",
    "    return(s)\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def proc_txt(inFile):\n",
    "    with open(inFile, mode='r') as readFile: \n",
    "        lines = readFile.read().splitlines() # read input file and split lines\n",
    "    with open(inFile+'out.csv',mode='w') as writeFile:\n",
    "        writer = csv.writer(writeFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        #lines.pop(0) # remove title\n",
    "        for index,l in enumerate(lines):\n",
    "            line = process_line(l)\n",
    "            if len(line)==48:\n",
    "                writer.writerow(line)\n",
    "            last_length = len(line)\n",
    "            if len(line) != last_length:\n",
    "                print(f'error in length starting at index {index} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates df grouped by l_code,l_name,year from df with multiple years for columns\n",
    "def explode_df(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in range(0,df.shape[0]): # loop over rows\n",
    "        # grab location code and name for each row\n",
    "        l_code, l_name = df.loc[i,['番号','地点名']]\n",
    "        # figure out where in clean_data to insert new rows\n",
    "        if pd.isna(clean_data.index.max()):\n",
    "            # if empty, insert into first row\n",
    "            row_index = 0 \n",
    "        else:\n",
    "            # insert into row at bottom\n",
    "            row_index = clean_data.index.max()+1\n",
    "        # loops over the pairs of columns we want to explode into row entries\n",
    "        for index, item in enumerate(df.columns):\n",
    "            next = index + 1\n",
    "            if ((next < len(df.columns)) & (index > 1) & (index % 2 == 0)):\n",
    "                # lookup entries for new row from in df\n",
    "                day = df.iloc[i,index]\n",
    "                rm = df.iloc[i,next]\n",
    "                clean_data.loc[row_index,['l_code','l_name','year','day','rm']] = \\\n",
    "                    [l_code,l_name,item,day,rm]\n",
    "                row_index = row_index + 1\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize empty data frame to hold processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.DataFrame([],columns=['l_code','l_name','year','day','rm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run the proc_txt calls once to generate .txtout files\n",
    "proc_txt('data/raw/s4_53_1.txt')\n",
    "proc_txt('data/raw/s4_53_2.txt')\n",
    "df1 = pd.read_csv('data/raw/s4_53_1.txtout.csv')\n",
    "df2 = pd.read_csv('data/raw/s4_53_2.txtout.csv')\n",
    "\n",
    "d1 = pd.concat([df1,df2]); \n",
    "d1 = d1.loc[d1['番号'].isnull()==False,:] # sanity check for rows w/missing location\n",
    "explode_df(d1) # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different proc_text function for pages that don't paste nicely like the 56 pages (so all of the other pages, well minus the last one which requires yet another function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# for the first page of 2016+ text file and associated cols file\n",
    "# but only round 2 is shown\n",
    "def process_file_and_cols(filePrefix):\n",
    "    with open(os.path.join(filePrefix+'.txt'), mode='r') as readFile: \n",
    "        lines = readFile.read().splitlines()\n",
    "    with open(os.path.join(filePrefix+'.txtout.csv'),mode='w') as writeFile:\n",
    "         writer = csv.writer(writeFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "         #lines.pop(0) # remove title\n",
    "         for index,l in enumerate(lines):\n",
    "            line = process_line(l)\n",
    "            if len(line) > 40:\n",
    "                line.pop(0)\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "    with open(os.path.join(filePrefix+'cols.txt'), mode='r') as readFile: \n",
    "        lines = readFile.read().splitlines()\n",
    "    with open(os.path.join(filePrefix+'cols.txtout.csv'),mode='w') as writeFile:\n",
    "         writer = csv.writer(writeFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "         #lines.pop(0) # remove title\n",
    "         for index,l in enumerate(lines):\n",
    "            line = process_line(l)\n",
    "            if len(line) > 40:\n",
    "                line.pop(0)\n",
    "            writer.writerow(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_and_cols('data/raw/s4_76_1')\n",
    "df1 = pd.read_csv('data/raw/s4_76_1.txtout.csv')\n",
    "df2 = pd.read_csv('data/raw/s4_76_1cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1); d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_and_cols('data/raw/s4_76_2')\n",
    "df1 = pd.read_csv('data/raw/s4_76_2.txtout.csv')\n",
    "df2 = pd.read_csv('data/raw/s4_76_2cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1); d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1995'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_and_cols('data/raw/s4_96_1')\n",
    "df1 = pd.read_csv('data/raw/s4_96_1.txtout.csv')\n",
    "df2 = pd.read_csv('data/raw/s4_96_1cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1); d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file_and_cols('data/raw/s4_96_2')\n",
    "df1 = pd.read_csv('data/raw/s4_96_2.txtout.csv')\n",
    "df2 = pd.read_csv('data/raw/s4_96_2cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1); d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pages of 2016+ text file and associated cols file\n",
    "def process_last_pages(filePrefix):\n",
    "    with open(os.path.join(filePrefix+'.txt'), mode='r') as readFile: \n",
    "        lines = readFile.read().splitlines()\n",
    "    with open(os.path.join(filePrefix+'.txtout.csv'),mode='w') as writeFile:\n",
    "         writer = csv.writer(writeFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "         #lines.pop(0) # remove title\n",
    "         for index,l in enumerate(lines):\n",
    "            line = process_line(l)\n",
    "            if len(line) > 14:\n",
    "                line.pop(0)\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "    with open(os.path.join(filePrefix+'cols.txt'), mode='r') as readFile: \n",
    "        lines = readFile.read().splitlines()\n",
    "    with open(os.path.join(filePrefix+'cols.txtout.csv'),mode='w') as writeFile:\n",
    "         writer = csv.writer(writeFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "         #lines.pop(0) # remove title\n",
    "         for index,l in enumerate(lines):\n",
    "            line = process_line(l)\n",
    "            if len(line) > 14:\n",
    "                line.pop(0)\n",
    "            writer.writerow(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process first last page\n",
    "process_last_pages('data/raw/s4_16_1')\n",
    "df1 = pd.read_csv('data/raw/s4_16_1.txtout.csv').iloc[:,0:6]\n",
    "df2 = pd.read_csv('data/raw/s4_16_1cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1);d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process second last page\n",
    "process_last_pages('data/raw/s4_16_2')\n",
    "df1 = pd.read_csv('data/raw/s4_16_2.txtout.csv').iloc[:,0:6]\n",
    "df2 = pd.read_csv('data/raw/s4_16_2cols.txtout.csv')\n",
    "d1 = pd.concat([df2,df1],axis=1);d1.head()\n",
    "explode_df(d1)  # append new data after last existing row in clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_code</th>\n",
       "      <th>l_name</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>rm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [l_code, l_name, year, day, rm]\n",
       "Index: []"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.loc[clean_data['l_code'].isnull()==True,:] # check for null data by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.drop_duplicates(subset=['l_code','year'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018', '1953')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some sanity checks to make sure all data is included\n",
    "clean_data.year.max(), clean_data.year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l_code    0\n",
       "l_name    0\n",
       "year      0\n",
       "day       0\n",
       "rm        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "clean_data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('data/flowering.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data.year.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_code</th>\n",
       "      <th>l_name</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>rm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>401</td>\n",
       "      <td>稚内</td>\n",
       "      <td>1953</td>\n",
       "      <td>521</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>776</td>\n",
       "      <td>洲本</td>\n",
       "      <td>1953</td>\n",
       "      <td>330</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>426</td>\n",
       "      <td>浦河</td>\n",
       "      <td>1953</td>\n",
       "      <td>509</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>772</td>\n",
       "      <td>大阪</td>\n",
       "      <td>1953</td>\n",
       "      <td>402</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>770</td>\n",
       "      <td>神戸</td>\n",
       "      <td>1953</td>\n",
       "      <td>404</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l_code l_name  year  day rm\n",
       "0       401     稚内  1953  521  6\n",
       "1656    776     洲本  1953  330  8\n",
       "230     426     浦河  1953  509  6\n",
       "1633    772     大阪  1953  402  8\n",
       "1610    770     神戸  1953  404  8"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.sort_values('year').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
